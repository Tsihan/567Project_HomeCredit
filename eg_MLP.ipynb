{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-29T05:34:35.398541Z",
     "start_time": "2024-04-29T05:34:35.396121Z"
    }
   },
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:34:35.405090Z",
     "start_time": "2024-04-29T05:34:35.402123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_table_dtypes(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # implement here all desired dtypes for tables\n",
    "    # the following is just an example\n",
    "    for col in df.columns:\n",
    "        # last letter of column name will help you determine the type\n",
    "        if col[-1] in (\"P\", \"A\"):\n",
    "            df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n",
    "\n",
    "    return df\n",
    "\n",
    "def convert_strings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.columns:  \n",
    "        if df[col].dtype.name in ['object', 'string']:\n",
    "            df[col] = df[col].astype(\"string\").astype('category')\n",
    "            current_categories = df[col].cat.categories\n",
    "            new_categories = current_categories.to_list() + [\"Unknown\"]\n",
    "            new_dtype = pd.CategoricalDtype(categories=new_categories, ordered=True)\n",
    "            df[col] = df[col].astype(new_dtype)\n",
    "    return df"
   ],
   "id": "6e27533f88ade988",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:34:38.115669Z",
     "start_time": "2024-04-29T05:34:35.406067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_basetable = pl.read_csv(\"train/train_base.csv\")\n",
    "train_static = pl.concat(\n",
    "    [\n",
    "        pl.read_csv(\"train/train_static_0_0.csv\").pipe(set_table_dtypes),\n",
    "        pl.read_csv(\"train/train_static_0_1.csv\").pipe(set_table_dtypes),\n",
    "    ],\n",
    "    how=\"vertical_relaxed\",\n",
    ")\n",
    "train_static_cb = pl.read_csv(\"train/train_static_cb_0.csv\").pipe(set_table_dtypes)\n",
    "train_person_1 = pl.read_csv(\"train/train_person_1.csv\").pipe(set_table_dtypes) \n",
    "train_credit_bureau_b_2 = pl.read_csv(\"train/train_credit_bureau_b_2.csv\").pipe(set_table_dtypes) "
   ],
   "id": "1d528121811f71d8",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:34:38.125032Z",
     "start_time": "2024-04-29T05:34:38.116937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_basetable = pl.read_csv(\"test/test_base.csv\")\n",
    "test_static = pl.concat(\n",
    "    [\n",
    "        pl.read_csv(\"test/test_static_0_0.csv\").pipe(set_table_dtypes),\n",
    "        pl.read_csv(\"test/test_static_0_1.csv\").pipe(set_table_dtypes),\n",
    "        pl.read_csv(\"test/test_static_0_2.csv\").pipe(set_table_dtypes),\n",
    "    ],\n",
    "    how=\"vertical_relaxed\",\n",
    ")\n",
    "test_static_cb = pl.read_csv(\"test/test_static_cb_0.csv\").pipe(set_table_dtypes)\n",
    "test_person_1 = pl.read_csv(\"test/test_person_1.csv\").pipe(set_table_dtypes) \n",
    "test_credit_bureau_b_2 = pl.read_csv(\"test/test_credit_bureau_b_2.csv\").pipe(set_table_dtypes) "
   ],
   "id": "140f038523f88e8",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:34:38.563389Z",
     "start_time": "2024-04-29T05:34:38.125532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We need to use aggregation functions in tables with depth > 1, so tables that contain num_group1 column or \n",
    "# also num_group2 column.\n",
    "train_person_1_feats_1 = train_person_1.group_by(\"case_id\").agg(\n",
    "    pl.col(\"mainoccupationinc_384A\").max().alias(\"mainoccupationinc_384A_max\"),\n",
    "    (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"mainoccupationinc_384A_any_selfemployed\")\n",
    ")\n",
    "\n",
    "# Here num_group1=0 has special meaning, it is the person who applied for the loan.\n",
    "train_person_1_feats_2 = train_person_1.select([\"case_id\", \"num_group1\", \"housetype_905L\"]).filter(\n",
    "    pl.col(\"num_group1\") == 0\n",
    ").drop(\"num_group1\").rename({\"housetype_905L\": \"person_housetype\"})\n",
    "\n",
    "# Here we have num_goup1 and num_group2, so we need to aggregate again.\n",
    "train_credit_bureau_b_2_feats = train_credit_bureau_b_2.group_by(\"case_id\").agg(\n",
    "    pl.col(\"pmts_pmtsoverdue_635A\").max().alias(\"pmts_pmtsoverdue_635A_max\"),\n",
    "    (pl.col(\"pmts_dpdvalue_108P\") > 31).max().alias(\"pmts_dpdvalue_108P_over31\")\n",
    ")\n",
    "\n",
    "# We will process in this examples only A-type and M-type columns, so we need to select them.\n",
    "selected_static_cols = []\n",
    "for col in train_static.columns:\n",
    "    if col[-1] in (\"A\", \"M\"):\n",
    "        selected_static_cols.append(col)\n",
    "print(selected_static_cols)\n",
    "\n",
    "selected_static_cb_cols = []\n",
    "for col in train_static_cb.columns:\n",
    "    if col[-1] in (\"A\", \"M\"):\n",
    "        selected_static_cb_cols.append(col)\n",
    "print(selected_static_cb_cols)\n",
    "\n",
    "# Join all tables together.\n",
    "data = train_basetable.join(\n",
    "    train_static.select([\"case_id\"]+selected_static_cols), how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    train_static_cb.select([\"case_id\"]+selected_static_cb_cols), how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    train_person_1_feats_1, how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    train_person_1_feats_2, how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    train_credit_bureau_b_2_feats, how=\"left\", on=\"case_id\"\n",
    ")"
   ],
   "id": "6ef64ce350d4c7fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amtinstpaidbefduel24m_4187115A', 'annuity_780A', 'annuitynextmonth_57A', 'avginstallast24m_3658937A', 'avglnamtstart24m_4525187A', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'credamount_770A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'disbursedcredamount_1113A', 'downpmt_116A', 'inittransactionamount_650A', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastapprcredamount_781A', 'lastcancelreason_561M', 'lastotherinc_902A', 'lastotherlnsexpense_631A', 'lastrejectcommoditycat_161M', 'lastrejectcommodtypec_5251769M', 'lastrejectcredamount_222A', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M', 'maininc_215A', 'maxannuity_159A', 'maxannuity_4075009A', 'maxdebt4_972A', 'maxinstallast24m_3658928A', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'maxpmtlast3m_4525190A', 'previouscontdistrict_112M', 'price_1097A', 'sumoutstandtotal_3546847A', 'sumoutstandtotalest_4493215A', 'totaldebt_9A', 'totalsettled_863A', 'totinstallast1m_4525188A']\n",
      "['description_5085714M', 'education_1103M', 'education_88M', 'maritalst_385M', 'maritalst_893M', 'pmtaverage_3A', 'pmtaverage_4527227A', 'pmtaverage_4955615A', 'pmtssum_45A']\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:34:38.569175Z",
     "start_time": "2024-04-29T05:34:38.564671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_person_1_feats_1 = test_person_1.group_by(\"case_id\").agg(\n",
    "    pl.col(\"mainoccupationinc_384A\").max().alias(\"mainoccupationinc_384A_max\"),\n",
    "    (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"mainoccupationinc_384A_any_selfemployed\")\n",
    ")\n",
    "\n",
    "test_person_1_feats_2 = test_person_1.select([\"case_id\", \"num_group1\", \"housetype_905L\"]).filter(\n",
    "    pl.col(\"num_group1\") == 0\n",
    ").drop(\"num_group1\").rename({\"housetype_905L\": \"person_housetype\"})\n",
    "\n",
    "test_credit_bureau_b_2_feats = test_credit_bureau_b_2.group_by(\"case_id\").agg(\n",
    "    pl.col(\"pmts_pmtsoverdue_635A\").max().alias(\"pmts_pmtsoverdue_635A_max\"),\n",
    "    (pl.col(\"pmts_dpdvalue_108P\") > 31).max().alias(\"pmts_dpdvalue_108P_over31\")\n",
    ")\n",
    "\n",
    "data_submission = test_basetable.join(\n",
    "    test_static.select([\"case_id\"]+selected_static_cols), how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    test_static_cb.select([\"case_id\"]+selected_static_cb_cols), how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    test_person_1_feats_1, how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    test_person_1_feats_2, how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    test_credit_bureau_b_2_feats, how=\"left\", on=\"case_id\"\n",
    ")"
   ],
   "id": "46765b2280ad9010",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:34:40.200227Z",
     "start_time": "2024-04-29T05:34:38.569747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "case_ids = data[\"case_id\"].unique().shuffle(seed=1)\n",
    "case_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=1)\n",
    "case_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=1)\n",
    "\n",
    "cols_pred = []\n",
    "for col in data.columns:\n",
    "    if col[-1].isupper() and col[:-1].islower():\n",
    "        cols_pred.append(col)\n",
    "\n",
    "print(cols_pred)\n",
    "\n",
    "def from_polars_to_pandas(case_ids: pl.DataFrame) -> pl.DataFrame:\n",
    "    return (\n",
    "        data.filter(pl.col(\"case_id\").is_in(case_ids))[[\"case_id\", \"WEEK_NUM\", \"target\"]].to_pandas(),\n",
    "        data.filter(pl.col(\"case_id\").is_in(case_ids))[cols_pred].to_pandas(),\n",
    "        data.filter(pl.col(\"case_id\").is_in(case_ids))[\"target\"].to_pandas()\n",
    "    )\n",
    "\n",
    "base_train, X_train, y_train = from_polars_to_pandas(case_ids_train)\n",
    "base_valid, X_valid, y_valid = from_polars_to_pandas(case_ids_valid)\n",
    "base_test, X_test, y_test = from_polars_to_pandas(case_ids_test)\n",
    "\n",
    "for df in [X_train, X_valid, X_test]:\n",
    "    df = convert_strings(df)"
   ],
   "id": "319f2edbafc735cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amtinstpaidbefduel24m_4187115A', 'annuity_780A', 'annuitynextmonth_57A', 'avginstallast24m_3658937A', 'avglnamtstart24m_4525187A', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'credamount_770A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'disbursedcredamount_1113A', 'downpmt_116A', 'inittransactionamount_650A', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastapprcredamount_781A', 'lastcancelreason_561M', 'lastotherinc_902A', 'lastotherlnsexpense_631A', 'lastrejectcommoditycat_161M', 'lastrejectcommodtypec_5251769M', 'lastrejectcredamount_222A', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M', 'maininc_215A', 'maxannuity_159A', 'maxannuity_4075009A', 'maxdebt4_972A', 'maxinstallast24m_3658928A', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'maxpmtlast3m_4525190A', 'previouscontdistrict_112M', 'price_1097A', 'sumoutstandtotal_3546847A', 'sumoutstandtotalest_4493215A', 'totaldebt_9A', 'totalsettled_863A', 'totinstallast1m_4525188A', 'description_5085714M', 'education_1103M', 'education_88M', 'maritalst_385M', 'maritalst_893M', 'pmtaverage_3A', 'pmtaverage_4527227A', 'pmtaverage_4955615A', 'pmtssum_45A']\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T05:34:40.202469Z",
     "start_time": "2024-04-29T05:34:40.200871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Train: {X_train.shape}\")\n",
    "print(f\"Valid: {X_valid.shape}\")\n",
    "print(f\"Test: {X_test.shape}\")"
   ],
   "id": "6fba15a6f262f6cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (915995, 48)\n",
      "Valid: (305332, 48)\n",
      "Test: (305332, 48)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-29T05:34:45.122708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 对类别特征进行独热编码\n",
    "X_train_encoded = pd.get_dummies(X_train)\n",
    "X_valid_encoded = pd.get_dummies(X_valid)\n",
    "X_test_encoded = pd.get_dummies(X_test)\n",
    "\n",
    "\n",
    "# 确保所有数据集具有相同的列\n",
    "X_train_encoded, X_valid_encoded = X_train_encoded.align(X_valid_encoded, join='outer', axis=1)\n",
    "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='outer', axis=1)\n",
    "X_valid_encoded, X_test_encoded = X_valid_encoded.align(X_test_encoded, join='outer', axis=1)\n",
    "\n",
    "# 用 0 填充 NaN 值\n",
    "X_train_encoded = X_train_encoded.fillna(0)\n",
    "X_valid_encoded = X_valid_encoded.fillna(0)\n",
    "X_test_encoded = X_test_encoded.fillna(0)\n",
    "# 训练MLP模型\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        # 定义全连接层\n",
    "        self.fc1 = nn.Linear(num_features, 128)  \n",
    "        self.relu1 = nn.ReLU()  \n",
    "        self.fc2 = nn.Linear(128, 64)  \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  \n",
    "        x = self.relu1(x) \n",
    "        x = self.fc2(x)  \n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        return torch.sigmoid(x)  # 使用sigmoid函数输出预测的概率\n",
    "\n",
    "\n",
    "class CreditDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_encoded_callback = X_train_encoded.copy()\n",
    "X_train_encoded = scaler.fit_transform(X_train_encoded)\n",
    "X_valid_encoded = scaler.transform(X_valid_encoded)\n",
    "X_test_encoded = scaler.transform(X_test_encoded)\n",
    "\n",
    "train_dataset = CreditDataset(torch.tensor(X_train_encoded).float().unsqueeze(1), torch.tensor(y_train).float())\n",
    "valid_dataset = CreditDataset(torch.tensor(X_valid_encoded).float().unsqueeze(1), torch.tensor(y_valid).float())\n",
    "test_dataset = CreditDataset(torch.tensor(X_test_encoded).float().unsqueeze(1), torch.tensor(y_test).float())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use MPS (Metal Performance Shaders)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU if MPS is not available\n",
    "    \n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SimpleMLP(num_features=X_train_encoded.shape[1]).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(20):  # number of epochs\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            valid_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {valid_loss / len(valid_loader)}\")\n"
   ],
   "id": "34fb267d67893d20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 0.12990196641386992\n",
      "Epoch 2, Validation Loss: 0.12917613669987077\n",
      "Epoch 3, Validation Loss: 0.1279605252348516\n",
      "Epoch 4, Validation Loss: 0.12774009198759292\n",
      "Epoch 5, Validation Loss: 0.12774463219158594\n",
      "Epoch 6, Validation Loss: 0.12774571377059413\n",
      "Epoch 7, Validation Loss: 0.12898033357542957\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to calculate predictions\n",
    "def predict(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # Squeeze the outputs to remove the extra dimension and then detach and move to CPU\n",
    "            predictions.append(outputs.squeeze().detach().cpu().numpy())\n",
    "    return np.concatenate(predictions)\n",
    "\n",
    "predictions_train = predict(model, train_loader)\n",
    "predictions_valid = predict(model, valid_loader)\n",
    "predictions_test = predict(model, test_loader)\n",
    "\n",
    "\n",
    "base_train[\"score\"] = predictions_train\n",
    "base_valid[\"score\"] = predictions_valid\n",
    "base_test[\"score\"] = predictions_test\n",
    "\n",
    "print(f'The AUC score on the train set is: {roc_auc_score(base_train[\"target\"], base_train[\"score\"])}')\n",
    "print(f'The AUC score on the valid set is: {roc_auc_score(base_valid[\"target\"], base_valid[\"score\"])}')\n",
    "print(f'The AUC score on the test set is: {roc_auc_score(base_test[\"target\"], base_test[\"score\"])}')"
   ],
   "id": "bd752a6aa3b0b650",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n",
    "    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n",
    "        .sort_values(\"WEEK_NUM\")\\\n",
    "        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n",
    "        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n",
    "    \n",
    "    x = np.arange(len(gini_in_time))\n",
    "    y = gini_in_time\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "    y_hat = a*x + b\n",
    "    residuals = y - y_hat\n",
    "    res_std = np.std(residuals)\n",
    "    avg_gini = np.mean(gini_in_time)\n",
    "    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n",
    "\n",
    "stability_score_train = gini_stability(base_train)\n",
    "stability_score_valid = gini_stability(base_valid)\n",
    "stability_score_test = gini_stability(base_test)\n",
    "\n",
    "print(f'The stability score on the train set is: {stability_score_train}') \n",
    "print(f'The stability score on the valid set is: {stability_score_valid}') \n",
    "print(f'The stability score on the test set is: {stability_score_test}')  "
   ],
   "id": "35fc4f5e78904858",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_submission = data_submission[cols_pred].to_pandas()\n",
    "X_submission = convert_strings(X_submission)\n",
    "categorical_cols = X_train.select_dtypes(include=['category']).columns\n",
    "\n",
    "for col in categorical_cols:\n",
    "    train_categories = set(X_train[col].cat.categories)\n",
    "    submission_categories = set(X_submission[col].cat.categories)\n",
    "    new_categories = submission_categories - train_categories\n",
    "    X_submission.loc[X_submission[col].isin(new_categories), col] = \"Unknown\"\n",
    "    new_dtype = pd.CategoricalDtype(categories=train_categories, ordered=True)\n",
    "    X_train[col] = X_train[col].astype(new_dtype)\n",
    "    X_submission[col] = X_submission[col].astype(new_dtype)\n",
    "\n",
    "\n",
    "# 对类别特征进行独热编码\n",
    "X_submission_encoded = pd.get_dummies(X_submission)\n",
    "# 确保特征对齐\n",
    "X_submission_encoded = X_submission_encoded.reindex(columns=X_train_encoded_callback.columns, fill_value=0)\n",
    "# 用 0 填充 NaN 值\n",
    "X_submission_encoded = X_submission_encoded.fillna(0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_submission_encoded = scaler.fit_transform(X_submission_encoded)\n",
    "\n",
    "class SubmissionDataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "X_submission_encoded_tensor = torch.tensor(X_submission_encoded).float().unsqueeze(1)\n",
    "submission_dataset = SubmissionDataset(X_submission_encoded_tensor)\n",
    "submission_loader = DataLoader(submission_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "def predict_submission(model, submission_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs in submission_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions.extend(outputs.detach().cpu().numpy().flatten())  # Flatten to convert 2D to 1D\n",
    "    return np.array(predictions)\n",
    "\n",
    "y_submission_pred = predict_submission(model, submission_loader)\n"
   ],
   "id": "9e448ce1ed80ecf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"case_id\": data_submission[\"case_id\"].to_numpy(),\n",
    "    \"score\": y_submission_pred\n",
    "}).set_index('case_id')\n",
    "submission.to_csv(\"./submission_MLP.csv\")"
   ],
   "id": "3c53d0fb357e3ae0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
